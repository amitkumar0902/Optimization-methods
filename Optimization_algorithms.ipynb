{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization_algorithms.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TZudg18-_19"
      },
      "source": [
        "# BFGS (Broyden–Fletcher–Goldfarb–Shanno)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAGn2vSpvTFZ",
        "outputId": "65e0598b-0074-437f-9f75-a4abb9374868"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import numpy.linalg as ln\n",
        "import scipy as sp\n",
        "import scipy.optimize\n",
        "import numdifftools as nd\n",
        "# Objective function\n",
        "def f(x):\n",
        "    return 4*(x[0]**2) + (x[0] * x[1])+ x[1]**2 - x[0] - 3*x[1] +7\n",
        "\n",
        "def bfgs_method(f, x0, maxiter=None, epsi=10e-12):\n",
        "    # Derivative\n",
        "    fprime=nd.Gradient(f)\n",
        "    print(fprime)\n",
        "    if maxiter is None:\n",
        "        maxiter = len(x0) * 200\n",
        "    # initial values\n",
        "    k = 0\n",
        "    gfk = fprime(x0)\n",
        "    N = len(x0)\n",
        "    I = np.eye(N, dtype=int)\n",
        "    Hk = I\n",
        "    xk = x0\n",
        "    while ln.norm(gfk) > epsi and k < maxiter:\n",
        "        pk = -np.dot(Hk, gfk)\n",
        "        line_search = sp.optimize.line_search(f, fprime, xk, pk)\n",
        "        alpha_k = line_search[0]\n",
        "        xkp1 = xk + alpha_k * pk\n",
        "        sk = xkp1 - xk #delta X\n",
        "        xk = xkp1\n",
        "        gfkp1 = fprime(xkp1)\n",
        "        yk = gfkp1 - gfk  #delta gradient\n",
        "        gfk = gfkp1\n",
        "        k += 1\n",
        "        ro = 1.0 / (np.dot(yk, sk))\n",
        "        A1 = I - ro * sk[:, np.newaxis] * yk[np.newaxis, :]\n",
        "        A2 = I - ro * yk[:, np.newaxis] * sk[np.newaxis, :]\n",
        "        Hk = np.dot(A1, np.dot(Hk, A2)) + (ro * sk[:, np.newaxis] * sk[np.newaxis, :])\n",
        "    return (xk, k)\n",
        "result, k = bfgs_method(f, np.array([0,0]))\n",
        "print('The solution of bfgs algorithm is %s' % (result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<numdifftools.core.Gradient object at 0x7f167ea4b310>\n",
            "The solution of bfgs algorithm is [-0.06666667  1.53333333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wIaGfPybDWE"
      },
      "source": [
        "# Steepest Descent Algorithm for General Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgUYgAuJbEC3",
        "outputId": "35069506-f738-490f-a5d8-81ccf14e25d2"
      },
      "source": [
        "#!pip install numdifftools\n",
        "import numpy as np\n",
        "import numdifftools as nd\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def Steepest_Descent(x,eps=10e-8):\n",
        "\n",
        "    i = 0 \n",
        "    while True:\n",
        "        grad = nd.Gradient(func)(x[i]) \n",
        "        grad_norm = norm(grad,2)      \n",
        "        if grad_norm < eps:            \n",
        "            print(f\"minimum point {x[i]} is obtained at iteration {i}\")\n",
        "            break\n",
        "        _dir = -grad                   \n",
        "        H = nd.Hessian(func)(x[i])     \n",
        "        alpha = (_dir.T @ _dir) / (_dir.T @ H @ _dir) \n",
        "               \n",
        "        new_x = x[i] + alpha * _dir   \n",
        "        x.append(new_x)               \n",
        "       \n",
        "        i+=1                         \n",
        "    \n",
        "    return\n",
        "\n",
        "def func(x): \n",
        "    return ( ((x[0] - 2)**2)/8 + ((x[1]-3)**2)/12  ) \n",
        "\n",
        "x = []                  \n",
        "x.append([1,1])      \n",
        "Steepest_Descent(x)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minimum point [1.99999993 2.99999986] is obtained at iteration 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4e53iHLcWR6"
      },
      "source": [
        "#Newton's Minimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnoO8E0SCP8l",
        "outputId": "7576bd75-25f3-4bb5-dfac-5c04111a0231"
      },
      "source": [
        "import numpy as np\n",
        "from sympy import *\n",
        "def hessian(f,xi):\n",
        "  symb=[k for k in [x,y,z,v] if f.count(k)!=0]\n",
        "  n=len(symb)\n",
        "  M=np.zeros((n,n))\n",
        "  for p in range(0,n):\n",
        "    for j in range(0,n):\n",
        "      holder=diff(f,symb[p],symb[j])\n",
        "      for q in range(0,n):\n",
        "        holder=holder.subs(symb[q],xi[q])\n",
        "      M[p][j]=holder\n",
        "  M=np.linalg.inv(M)\n",
        "  return M\n",
        "def grad(f,xi):\n",
        "  symb=[k for k in [x,y,z,v] if f.count(k)!=0]\n",
        "  n=len(symb)\n",
        "  M=np.zeros(n)\n",
        "  for p in range(0,n):\n",
        "    holder=diff(f,symb[p])\n",
        "    for q in range(0,n):\n",
        "      holder=holder.subs(symb[q],xi[q])\n",
        "    M[p]=holder\n",
        "  M=M.reshape(n,1)\n",
        "  return M\n",
        "def Newton_min(f,tol=10e-5):\n",
        "  symb=[k for k in [x,y,z,v] if f.count(k)!=0]\n",
        "  n=len(symb)\n",
        "  x0=np.array([28,10,6]).reshape(n,1)# initial value\n",
        "  i=0\n",
        "  while True:\n",
        "    hess=hessian(f,x0)        \n",
        "    gradient=grad(f,x0)       \n",
        "    xk=x0-np.dot(hess,gradient)  \n",
        "    error=np.linalg.norm(x0-xk)\n",
        "    x0=xk\n",
        "    if i>200:\n",
        "      break\n",
        "    if error<tol:\n",
        "      break\n",
        "    i+=1\n",
        "  return x0,i\n",
        "\n",
        "\n",
        "x,y,z,v=symbols('x y z v')       \n",
        "f=(x-30)**4+2*(y-12)**2+80*(z-5)**6           \n",
        "X,i=Newton_min(f)\n",
        "print(\"Iteration \",i)\n",
        "print(\"The minima point is : \",X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration  35\n",
            "The minima point is :  [[29.99999908]\n",
            " [12.        ]\n",
            " [ 5.00032452]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiDndxMbc_RO"
      },
      "source": [
        "# Steepest Descent for Quadatic Form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmOrZjZdc09o"
      },
      "source": [
        "def f(x,Q,b,c):\n",
        "    return (0.5*(x.T).dot(Q.dot(x))+(x.T).dot(b)+c)\n",
        "\n",
        "def grad_f(x,Q,b,c):\n",
        "    return (Q.dot(x)+b)\n",
        "\n",
        "def SteepestDescent(x,Q,b,c):\n",
        "    while True:\n",
        "        temp=x\n",
        "        g=grad_f(x,Q,b,c)\n",
        "        alpha=((g.T).dot(g))/((g.T).dot(Q.dot(g)))\n",
        "        x=x-alpha*grad_f(x,Q,b,c)\n",
        "        if (np.linalg.norm(x-temp)<1e-12):\n",
        "            break\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQqM3yzfc2-1",
        "outputId": "d3c786ed-e64e-414f-8029-78dffb75a2eb"
      },
      "source": [
        "x = np.array([[0.0],[0.0]])\n",
        "Q = np.array([[2.0, 5.0], [5.0, 15.0]])\n",
        "b = np.array([[1.0], [2.0]])  \n",
        "c = 8\n",
        "x=SteepestDescent(x,Q,b,c)\n",
        "print(\"The optimum point is : \",x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The optimum point is :  [[-1. ]\n",
            " [ 0.2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_XmD_VBFoMx"
      },
      "source": [
        "# Conjugate Gradient method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IS7VzFAFtkE",
        "outputId": "94bbec51-526e-4e1e-cd8c-5175b6c7a14e"
      },
      "source": [
        "import numpy as np\n",
        "A=np.array([[8,1],[1,2]])\n",
        "#print(Q)\n",
        "b=np.array([1,3])\n",
        "x=np.array([0,0])\n",
        "\n",
        "def CJ(A,b,x):\n",
        "    r = b - np.dot(A, x)\n",
        "    p = r\n",
        "    rsold = np.dot(np.transpose(r), r)\n",
        "    for i in range(len(b)):\n",
        "        Ap = np.dot(A, p)\n",
        "        alpha = rsold / np.dot(np.transpose(p), Ap)\n",
        "        x = x + np.dot(alpha, p)\n",
        "        r = r - np.dot(alpha, Ap)\n",
        "        rsnew = np.dot(np.transpose(r), r)\n",
        "        if np.sqrt(rsnew) < 1e-12:\n",
        "            break\n",
        "        p = r + (rsnew/rsold)*p\n",
        "        rsold = rsnew\n",
        "        #print(x)\n",
        "    return x\n",
        "\n",
        "x=CJ(A,b,x)\n",
        "print(\"solution is \",x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "solution is  [-0.06666667  1.53333333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQWN5Bb40Lcd"
      },
      "source": [
        "### Rank One"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FZ_felP0VsY",
        "outputId": "1f47e250-0c99-4fb4-ec8c-9d5d3d703726"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as ln\n",
        "import scipy as sp\n",
        "import scipy.optimize\n",
        "import numdifftools as nd\n",
        "\n",
        "# Objective function\n",
        "def f(x):\n",
        "    \n",
        "\n",
        "def rank_one_method(f, x0, maxiter=None, epsi=10e-6):\n",
        "    fprime=nd.Gradient(f)\n",
        "    if maxiter is None:\n",
        "        maxiter = len(x0) * 200\n",
        "    k = 0\n",
        "    gfk = fprime(x0)\n",
        "    N = len(x0)\n",
        "    I = np.eye(N, dtype=int)\n",
        "    Hk = I\n",
        "    xk = x0\n",
        "    while ln.norm(gfk) > epsi and k < maxiter:\n",
        "        pk = -np.dot(Hk, gfk)\n",
        "        line_search = sp.optimize.line_search(f, fprime, xk, pk)\n",
        "        alpha_k = line_search[0]\n",
        "        xkp1 = xk + alpha_k * pk\n",
        "        sk = xkp1 - xk #delta X\n",
        "        xk = xkp1\n",
        "        gfkp1 = fprime(xkp1)\n",
        "        yk = gfkp1 - gfk  #delta gradient\n",
        "        gfk = gfkp1\n",
        "        \n",
        "        u = sk - Hk @ yk\n",
        "        Hk = Hk + (u @ u.T)/(u.T @ yk)\n",
        "        \n",
        "        k += 1\n",
        "    return (xk, k)\n",
        "\n",
        "\n",
        "result, k = rank_one_method(f, np.array([0, 0]), 2)\n",
        "\n",
        "print('minima point: %s' % (result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minima point: [-0.02268802  1.61892619]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdTReQyi0eNJ"
      },
      "source": [
        "### Rank Two or DFP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7GoDzUQ0mSy",
        "outputId": "b3764112-a47c-4df6-b9ec-857497bd2de1"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as ln\n",
        "import scipy as sp\n",
        "import scipy.optimize\n",
        "import numdifftools as nd\n",
        "\n",
        "# Objective function\n",
        "def f(x):\n",
        "    return 4*(x[0]**2) + (x[0] * x[1])+ x[1]**2 - x[0] - 3*x[1] +7\n",
        "\n",
        "\n",
        "def rank_two_method(f, x0, maxiter=None, epsi=10e-2):\n",
        "   \n",
        "    fprime=nd.Gradient(f)\n",
        "    if maxiter is None:\n",
        "        maxiter = len(x0) * 200\n",
        "\n",
        "    k = 0\n",
        "    gfk = fprime(x0)\n",
        "    N = len(x0)\n",
        "    I = np.eye(N, dtype=int)\n",
        "    Hk = I\n",
        "    xk = x0\n",
        "    while ln.norm(gfk) > epsi and k < maxiter:\n",
        "        pk = -np.dot(Hk, gfk)\n",
        "\n",
        "        line_search = sp.optimize.line_search(f, fprime, xk, pk)\n",
        "        alpha_k = line_search[0]\n",
        "        xkp1 = xk + alpha_k * pk\n",
        "        sk = xkp1 - xk \n",
        "        xk = xkp1\n",
        "        gfkp1 = fprime(xkp1)\n",
        "        yk = gfkp1 - gfk  \n",
        "        gfk = gfkp1\n",
        "        \n",
        "        u = Hk @ yk\n",
        "        Hk = Hk + (sk @ sk.T)/(sk.T @ yk) - (u @ u.T)/(yk.T @ u)\n",
        "        \n",
        "        k += 1\n",
        "    return (xk, k)\n",
        "\n",
        "\n",
        "result, k = rank_two_method(f, np.array([0, 0]), 2)\n",
        "print(' The soultion is: %s' % (result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The soultion is: [-0.07523734  1.50419304]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}